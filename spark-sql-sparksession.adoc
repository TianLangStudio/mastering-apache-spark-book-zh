== [[SparkSession]] `SparkSession` -- Spark SQL的入口

`SparkSession` 是Spark SQL的入口--使用Spark SQL时需要创建的第一个对象.

NOTE: 在Spark 2.0将 link:spark-sql-sqlcontext.adoc[SQLContext] 和 link:spark-sql-hive-integration.adoc[HiveContext] 统一放在了SparkSession里.

你可以使用 <<builder, SparkSession.builder>>  创建 `SparkSession` 对象.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .appName("My Spark Application")  // 应用名称 可以不设置 如果你不设置会自动生成一个
  .master("local[*]")               // 应用运行模式  local standalone mesos yarn
  .enableHiveSupport()              // 启用Hive支持
  .config("spark.sql.warehouse.dir", "target/spark-warehouse") //设置仓库地址 后面有介绍
  .getOrCreate
----

调用<<stop, stop>> 方法关闭`SparkSession`  .

[source, scala]
----
spark.stop
----

在一个Spark应用里可以起多个``SparkSession``.

`SparkSession` 内部维护了一个link:spark-sparkcontext.adoc[SparkContext] 对象 和一个可选的<<SharedState, SharedState>> 对象 用于多个SparkSession对象间共享数据.

.`SparkSession` 静态方法和对象方法
[cols="1,2",options="header",width="100%"]
|===
| 方法 | 描述
| <<builder, builder>> | 用于创建`SparkSession` 对象
| <<version, version>> | 获取当前Spark的版本.
| <<implicits, implicits>> | 包含将Scala对象转换为DataSet的隐式转换 可以通过 导入 `import spark.implicits._` 使用.
| <<emptyDataset, emptyDataset[T]>> | 创建一个空的 `Dataset[T]`.
| <<range, range>> | 创建一个 `Dataset[Long]`.
| <<sql, sql>> | 执行sql返回一个`DataFrame`.
| <<udf, udf>> | 用于设置用户自定定义的ＳＱＬ　函数(UDFs).
| <<table, table>> | 将一个表映射为一个`DataFrame`对象
| <<catalog, catalog>> | 用于访问结构化数据的元信息（结构描述信息）
| <<read, read>> | 用于读取外部存储系统的文件或数据并生成 `DataFrame`　对象.
| <<conf, conf>> | 获取当前的配置信息.
| <<readStream, readStream>> | 用于读取流失数据.
| <<streams, streams>> | 使用结构化流式查询.
| <<newSession, newSession>> | 创建一个新的 `SparkSession`.
| <<stop, stop>> | 关闭 `SparkSession`.
|===

[TIP]
====
使用配置项 link:spark-sql-settings.adoc#spark_sql_warehouse_dir[spark.sql.warehouse.dir]
可以改变Hive `hive.metastore.warehouse.dir` 配置项值
　

|===
|更多参考信息：
|<<SharedState, SharedState>>
|https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin[Hive元数据管理].
|===

====

=== [[builder]] 创建 `SparkSession` 对象

[source, scala]
----
builder(): Builder
----

`builder` 创建一个 link:spark-sql-sparksession-builder.adoc[Builder]　对象，你可以用它创建Spark Session对像.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val builder = SparkSession.builder
----



=== [[version]] 查看Ｓｐａｒｋ版本信息 -- `version` 方法

[source, scala]
----
version: String
----

`version` 返回Ｓｐａｒｋ版本信息


=== [[implicits]] 隐式转换对象

用于scala编程的隐式转换 可以方便的将scala对象转换为 link:spark-sql-dataset.adoc[Datasets], link:spark-sql-dataframe.adoc[DataFrames] 和 link:spark-sql-columns.adoc[Columns].
并为scala "基础类型(Int,Double)" 定义了 link:spark-sql-Encoder.adoc[编码器].

[NOTE]
====
使用 `import spark.implicits._` 导入隐式转换

[source, scala]
----
val spark = SparkSession.builder.getOrCreate()
import spark.implicits._
----
====

`implicits` 支持从  `RDD`  (作用域里存在 link:spark-sql-Encoder.adoc[encoder]) case classes,tuples,`Seq` 创建 `Dataset`.

`implicits` 还支持将 Scala的`Symbol` 和`$` 转换成 `Column`.

支持将包含Product类型(case类,元组)的`RDD`和`Seq`转换为`DataFrame`.
支持将包含`Int`, `Long` 和 `String` 的`RDD`转换为只有一个名为`_1`列的DataFrame`.

注意: 需要通过调用`toDF`方法将包含基本数据类型(`Int`, `Long`, `String`) 的RDD转换为DataFrame.

=== [[emptyDataset]] 方法`emptyDataset` -- 创建一个空的DataSet

[source, scala]
----
emptyDataset[T: Encoder]: Dataset[T]
----

`emptyDataset` 创建一个空的 link:spark-sql-dataset.adoc[Dataset].

[source, scala]
----
scala> val strings = spark.emptyDataset[String]
strings: org.apache.spark.sql.Dataset[String] = [value: string]

scala> strings.printSchema
root
 |-- value: string (nullable = true)
----

`emptyDataset` 创建了一个  link:spark-sql-logical-plan-LocalRelation.adoc[`LocalRelation` 逻辑查询计划].

=== [[createDataset]] 方法`createDataset` -- 从集合或RDD创建DataSet

[source, scala]
----
createDataset[T : Encoder](data: Seq[T]): Dataset[T]
createDataset[T : Encoder](data: RDD[T]): Dataset[T]
----

在测试和实验时用`createDataset`可以方便的从集合类型创建 link:spark-sql-dataset.adoc[Dataset].

[source, scala]
----
scala> val one = spark.createDataset(Seq(1))
one: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> one.show
+-----+
|value|
+-----+
|    1|
+-----+
----

`createDataset` 创建了一个 link:spark-sql-logical-plan-LocalRelation.adoc[`LocalRelation` 逻辑查询计划] .

[小贴上]
====
你也可以使用:spark-sql-dataset.adoc#implicits[Scala隐式转换或者`toDS`方法] 完成类似的转换.

[source, scala]
----
val spark: SparkSession = ...
import spark.implicits._

scala> val one = Seq(1).toDS
one: org.apache.spark.sql.Dataset[Int] = [value: int]
----
====

`createDataset` 首先在link:spark-sql-schema.adoc[schema]的属性作用域(``AttributeReference``)里查找隐式的 link:spark-sql-Encoder.adoc#ExpressionEncoder[encoder].

注意: 当前只支持 unresolved link:spark-sql-Encoder.adoc#ExpressionEncoder[expression encoders].

然后会使用这个encoder表达式对集合或RDD做map操作返回DataSet.

=== [[range]] `range`方法 -- 创建只有一个Long类型列的DataSet

[source, scala]
----
range(end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long]
----

`range` 系列方法创建包含Long类型数据的 link:spark-sql-dataset.adoc[Dataset].

[source, scala]
----
scala> spark.range(start = 0, end = 4, step = 2, numPartitions = 5).show
+---+
| id|
+---+
|  0|
|  2|
+---+
----

注意: 如果不指定第四个参数(`numPartitions`) 默认使用
link:spark-sparkcontext.adoc#defaultParallelism[SparkContext.defaultParallelism].

`range` 创建了一个带`Range` link:spark-sql-LogicalPlan.adoc[逻辑计划] 和
`Encoders.LONG` link:spark-sql-Encoder.adoc[encoder] 的 `Dataset[Long]`
.

=== [[emptyDataFrame]] 方法`emptyDataFrame` -- 创建一个空的DataFrame

[source, scala]
----
emptyDataFrame: DataFrame
----

`emptyDataFrame` 创建一个没有行也没有列的空的 `DataFrame`.


=== [[createDataFrame]] 方法`createDataFrame` -- Creating DataFrames from RDDs with Explicit Schema --

[source, scala]
----
createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame
----

`createDataFrame` creates a `DataFrame` using `RDD[Row]` and the input `schema`. It is assumed that the rows in `rowRDD` all match the `schema`.

=== [[sql]] Executing SQL Queries -- `sql` method

[source, scala]
----
sql(sqlText: String): DataFrame
----

`sql` executes the `sqlText` SQL statement.

```
scala> sql("SHOW TABLES")
res0: org.apache.spark.sql.DataFrame = [tableName: string, isTemporary: boolean]

scala> sql("DROP TABLE IF EXISTS testData")
res1: org.apache.spark.sql.DataFrame = []

// Let's create a table to SHOW it
spark.range(10).write.option("path", "/tmp/test").saveAsTable("testData")

scala> sql("SHOW TABLES").show
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
| testdata|      false|
+---------+-----------+
```

Internally, it creates a link:spark-sql-dataset.adoc[Dataset] using the current `SparkSession` and a link:spark-sql-LogicalPlan.adoc[logical plan]. The plan is created by parsing the input `sqlText` using <<sessionState, sessionState.sqlParser>>.

CAUTION: FIXME See link:spark-sql-sqlcontext.adoc#sql[Executing SQL Queries].

=== [[udf]] Accessing UDF Registration Interface -- `udf` Attribute

[source, scala]
----
udf: UDFRegistration
----

`udf` attribute gives access to `UDFRegistration` that allows registering link:spark-sql-udfs.adoc[user-defined functions] for SQL-based query expressions.

[source, scala]
----
val spark: SparkSession = ...
spark.udf.register("myUpper", (s: String) => s.toUpperCase)

val strs = ('a' to 'c').map(_.toString).toDS
strs.registerTempTable("strs")

scala> sql("SELECT *, myUpper(value) UPPER FROM strs").show
+-----+-----+
|value|UPPER|
+-----+-----+
|    a|    A|
|    b|    B|
|    c|    C|
+-----+-----+
----

Internally, it is an alias for link:spark-sql-sessionstate.adoc#udf[SessionState.udf].

=== [[table]] Creating DataFrames from Tables -- `table` method

[source, scala]
----
table(tableName: String): DataFrame
----

`table` creates a link:spark-sql-dataframe.adoc[DataFrame] from records in the `tableName` table (if exists).

[source, scala]
----
val df = spark.table("mytable")
----

=== [[catalog]] Accessing Metastore -- `catalog` Attribute

[source, scala]
----
catalog: Catalog
----

`catalog` attribute is a (lazy) interface to the current metastore, i.e. link:spark-sql-Catalog.adoc[data catalog] (of relational entities like databases, tables, functions, table columns, and temporary views).

TIP: All methods in `Catalog` return `Datasets`.

[source, scala]
----
scala> spark.catalog.listTables.show
+------------------+--------+-----------+---------+-----------+
|              name|database|description|tableType|isTemporary|
+------------------+--------+-----------+---------+-----------+
|my_permanent_table| default|       null|  MANAGED|      false|
|              strs|    null|       null|TEMPORARY|       true|
+------------------+--------+-----------+---------+-----------+
----

Internally, `catalog` creates a link:spark-sql-Catalog.adoc#CatalogImpl[CatalogImpl] (referencing the current `SparkSession`).

=== [[read]] Accessing DataFrameReader -- `read` method

[source, scala]
----
read: DataFrameReader
----

`read` method returns a link:spark-sql-dataframereader.adoc[DataFrameReader] that is used to read data from external storage systems and load it into a `DataFrame`.

[source, scala]
----
val spark: SparkSession = // create instance
val dfReader: DataFrameReader = spark.read
----

=== [[conf]] Runtime Configuration -- `conf` attribute

[source, scala]
----
conf: RuntimeConfig
----

`conf` returns the current runtime configuration (as `RuntimeConfig`) that wraps link:spark-sql-SQLConf.adoc[SQLConf].

CAUTION: FIXME

=== [[sessionState]] `sessionState` Property

`sessionState` is a transient lazy value that represents the current link:spark-sql-sessionstate.adoc[SessionState].

NOTE: `sessionState` is a `private[sql]` value so you can only access it in a code inside `org.apache.spark.sql` package.

`sessionState` is a lazily-created value based on the internal link:spark-sql-settings.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] setting that can be:

* `org.apache.spark.sql.hive.HiveSessionState` for `hive`
* `org.apache.spark.sql.internal.SessionState` for `in-memory`

=== [[readStream]] `readStream` method

[source, scala]
----
readStream: DataStreamReader
----

`readStream` returns a new link:spark-sql-streaming-DataStreamReader.adoc[DataStreamReader].

=== [[streams]] `streams` Attribute

[source, scala]
----
streams: StreamingQueryManager
----

`streams` attribute gives access to link:spark-sql-streaming-StreamingQueryManager.adoc[StreamingQueryManager] (through link:spark-sql-sessionstate.adoc#streamingQueryManager[SessionState]).

[source, scala]
----
val spark: SparkSession = ...
spark.streams.active.foreach(println)
----

=== [[streamingQueryManager]] `streamingQueryManager` Attribute

`streamingQueryManager` is...

=== [[listenerManager]] `listenerManager` Attribute

`listenerManager` is...

=== [[ExecutionListenerManager]] `ExecutionListenerManager`

`ExecutionListenerManager` is...

=== [[functionRegistry]] `functionRegistry` Attribute

`functionRegistry` is...

=== [[experimentalMethods]] `experimentalMethods` Attribute

[source, scala]
----
experimental: ExperimentalMethods
----

`experimentalMethods` is an extension point with `ExperimentalMethods` that is a per-session collection of extra strategies and ``Rule[LogicalPlan]``s.

NOTE: `experimental` is used in link:spark-sql-SparkPlanner.adoc[SparkPlanner] and link:spark-sql-catalyst-Optimizer.adoc#SparkOptimizer[SparkOptimizer]. Hive and link:spark-sql-structured-streaming.adoc[Structured Streaming] use it for their own extra strategies and optimization rules.

=== [[newSession]] `newSession` method

[source, scala]
----
newSession(): SparkSession
----

`newSession` creates (starts) a new `SparkSession` (with the current link:spark-sparkcontext.adoc[SparkContext] and <<SharedState, SharedState>>).

[source, scala]
----
scala> println(sc.version)
2.0.0-SNAPSHOT

scala> val newSession = spark.newSession
newSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@122f58a
----

=== [[sharedState]] `sharedState` Attribute

`sharedState` is the current <<SharedState, SharedState>>. It is created lazily when first accessed.

=== [[SharedState]] `SharedState`

`SharedState` is an internal class that holds the shared state across active SQL sessions (as <<SparkSession, SparkSession>> instances) by sharing link:spark-sql-CacheManager.adoc[CacheManager], link:spark-webui-SQLListener.adoc[SQLListener], and link:spark-sql-ExternalCatalog.adoc[ExternalCatalog].

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.sql.internal.SharedState` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.internal.SharedState=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

`SharedState` requires a link:spark-sparkcontext.adoc[SparkContext] when created. It also adds `hive-site.xml` to link:spark-sparkcontext.adoc#hadoopConfiguration[Hadoop's `Configuration` in the current SparkContext] if found on CLASSPATH.

NOTE: `hive-site.xml` is an optional Hive configuration file when working with Hive in Spark.

The fully-qualified class name is `org.apache.spark.sql.internal.SharedState`.

`SharedState` is created lazily, i.e. when first accessed after <<creating-instance, `SparkSession` is created>>. It can happen when a <<newSession, new session is created>> or when the shared services are accessed. It is created with a link:spark-sparkcontext.adoc[SparkContext].

When created, `SharedState` sets `hive.metastore.warehouse.dir` to link:spark-sql-settings.adoc#spark_sql_warehouse_dir[spark.sql.warehouse.dir] if `hive.metastore.warehouse.dir` is not set or `spark.sql.warehouse.dir` is set. Otherwise, when `hive.metastore.warehouse.dir` is set and `spark.sql.warehouse.dir` is not, `spark.sql.warehouse.dir` gets set to `hive.metastore.warehouse.dir`. You should see the following INFO message in the logs:

```
INFO spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('[hiveWarehouseDir]').
```

You should see the following INFO message in the logs:

```
INFO SharedState: Warehouse path is '[warehousePath]'.
```

=== [[stop]] Stopping SparkSession -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` stops the `SparkSession`, i.e. link:spark-sparkcontext.adoc#stop[stops the underlying `SparkContext`].

=== [[creating-instance]] Creating `SparkSession` Instance

CAUTION: FIXME

=== [[baseRelationToDataFrame]] `baseRelationToDataFrame` Method

CAUTION: FIXME
