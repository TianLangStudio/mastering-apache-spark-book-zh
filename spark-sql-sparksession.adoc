== [[SparkSession]] `SparkSession` -- Spark SQL的入口

`SparkSession` 是Spark SQL的入口--使用Spark SQL时需要创建的第一个对象.

NOTE: 在Spark 2.0将 link:spark-sql-sqlcontext.adoc[SQLContext] 和 link:spark-sql-hive-integration.adoc[HiveContext] 统一放在了SparkSession里.

你可以使用 <<builder, SparkSession.builder>>  创建 `SparkSession` 对象.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .appName("My Spark Application")  // 应用名称 可以不设置 如果你不设置会自动生成一个
  .master("local[*]")               // 应用运行模式  local standalone mesos yarn
  .enableHiveSupport()              // 启用Hive支持
  .config("spark.sql.warehouse.dir", "target/spark-warehouse") //设置仓库地址 后面有介绍
  .getOrCreate
----

调用<<stop, stop>> 方法关闭`SparkSession`  .

[source, scala]
----
spark.stop
----

在一个Spark应用里可以起多个``SparkSession``.

`SparkSession` 内部维护了一个link:spark-sparkcontext.adoc[SparkContext] 对象 和一个可选的<<SharedState, SharedState>> 对象 用于多个SparkSession对象间共享数据.

.`SparkSession` 静态方法和对象方法
[cols="1,2",options="header",width="100%"]
|===
| 方法 | 描述
| <<builder, builder>> | 用于创建`SparkSession` 对象
| <<version, version>> | 获取当前Spark的版本.
| <<implicits, implicits>> | 包含将Scala对象转换为DataSet的隐式转换 可以通过 导入 `import spark.implicits._` 使用.
| <<emptyDataset, emptyDataset[T]>> | 创建一个空的 `Dataset[T]`.
| <<range, range>> | 创建一个 `Dataset[Long]`.
| <<sql, sql>> | 执行sql返回一个`DataFrame`.
| <<udf, udf>> | 用于设置用户自定定义的ＳＱＬ　函数(UDFs).
| <<table, table>> | 将一个表映射为一个`DataFrame`对象
| <<catalog, catalog>> | 用于访问结构化数据的元信息（结构描述信息）
| <<read, read>> | 用于读取外部存储系统的文件或数据并生成 `DataFrame`　对象.
| <<conf, conf>> | 获取当前的配置信息.
| <<readStream, readStream>> | 用于读取流失数据.
| <<streams, streams>> | 使用结构化流式查询.
| <<newSession, newSession>> | 创建一个新的 `SparkSession`.
| <<stop, stop>> | 关闭 `SparkSession`.
|===

[TIP]
====
使用配置项 link:spark-sql-settings.adoc#spark_sql_warehouse_dir[spark.sql.warehouse.dir]
可以改变Hive `hive.metastore.warehouse.dir` 配置项值
　

|===
|更多参考信息：
|<<SharedState, SharedState>>
|https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin[Hive元数据管理].
|===

====

=== [[builder]] 创建 `SparkSession` 对象

[source, scala]
----
builder(): Builder
----

`builder` 创建一个 link:spark-sql-sparksession-builder.adoc[Builder]　对象，你可以用它创建Spark Session对像.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val builder = SparkSession.builder
----



=== [[version]] 查看Ｓｐａｒｋ版本信息 -- `version` 方法

[source, scala]
----
version: String
----

`version` 返回Ｓｐａｒｋ版本信息


=== [[implicits]] 隐式转换对象

The `implicits` object is a helper class with the Scala implicit methods (aka _conversions_) to convert Scala objects to link:spark-sql-dataset.adoc[Datasets], link:spark-sql-dataframe.adoc[DataFrames] and link:spark-sql-columns.adoc[Columns]. It also defines link:spark-sql-Encoder.adoc[Encoders] for Scala's "primitive" types, e.g. `Int`, `Double`, `String`, and their products and collections.

[NOTE]
====
Import the implicits by `import spark.implicits._`.

[source, scala]
----
val spark = SparkSession.builder.getOrCreate()
import spark.implicits._
----
====

`implicits` object offers support for creating `Dataset` from `RDD` of any type (for which an link:spark-sql-Encoder.adoc[encoder] exists in scope), or case classes or tuples, and `Seq`.

`implicits` object also offers conversions from Scala's `Symbol` or `$` to `Column`.

It also offers conversions from `RDD` or `Seq` of `Product` types (e.g. case classes or tuples) to `DataFrame`. It has direct conversions from `RDD` of `Int`, `Long` and `String` to `DataFrame` with a single column name `_1`.

NOTE: It is only possible to call `toDF` methods on `RDD` objects of `Int`, `Long`, and `String` "primitive" types.

=== [[emptyDataset]] Creating Empty Dataset -- `emptyDataset` method

[source, scala]
----
emptyDataset[T: Encoder]: Dataset[T]
----

`emptyDataset` creates an empty link:spark-sql-dataset.adoc[Dataset] (assuming that future records being of type `T`).

[source, scala]
----
scala> val strings = spark.emptyDataset[String]
strings: org.apache.spark.sql.Dataset[String] = [value: string]

scala> strings.printSchema
root
 |-- value: string (nullable = true)
----

`emptyDataset` creates a  link:spark-sql-logical-plan-LocalRelation.adoc[`LocalRelation` logical query plan].

=== [[createDataset]] Creating Dataset from Local Collections and RDDs -- `createDataset` methods

[source, scala]
----
createDataset[T : Encoder](data: Seq[T]): Dataset[T]
createDataset[T : Encoder](data: RDD[T]): Dataset[T]
----

`createDataset` is an experimental API to create a link:spark-sql-dataset.adoc[Dataset] from a local Scala collection, i.e. `Seq[T]`, Java's `List[T]`, or a distributed `RDD[T]`.

[source, scala]
----
scala> val one = spark.createDataset(Seq(1))
one: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> one.show
+-----+
|value|
+-----+
|    1|
+-----+
----

`createDataset` creates a link:spark-sql-logical-plan-LocalRelation.adoc[`LocalRelation` logical query plan] (for the input `data` collection) or `LogicalRDD` (for the input `RDD[T]`).

[TIP]
====
You'd be better off using link:spark-sql-dataset.adoc#implicits[Scala implicits and `toDS` method] instead (that does this conversion automatically for you).

[source, scala]
----
val spark: SparkSession = ...
import spark.implicits._

scala> val one = Seq(1).toDS
one: org.apache.spark.sql.Dataset[Int] = [value: int]
----
====

Internally, `createDataset` first looks up the implicit link:spark-sql-Encoder.adoc#ExpressionEncoder[expression encoder] in scope to access the ``AttributeReference``s (of the link:spark-sql-schema.adoc[schema]).

NOTE: Only unresolved link:spark-sql-Encoder.adoc#ExpressionEncoder[expression encoders] are currently supported.

The expression encoder is then used to map elements (of the input `Seq[T]`) into a collection of link:spark-sql-InternalRow.adoc[InternalRows]. With the references and rows, `createDataset` returns a link:spark-sql-dataset.adoc[Dataset] with a link:spark-sql-logical-plan-LocalRelation.adoc[`LocalRelation` logical query plan].

=== [[range]] Creating Dataset With Single Long Column -- `range` methods

[source, scala]
----
range(end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long]
----

`range` family of methods create a link:spark-sql-dataset.adoc[Dataset] of `Long` numbers.

[source, scala]
----
scala> spark.range(start = 0, end = 4, step = 2, numPartitions = 5).show
+---+
| id|
+---+
|  0|
|  2|
+---+
----

NOTE: The three first variants (that do not specify `numPartitions` explicitly) use link:spark-sparkcontext.adoc#defaultParallelism[SparkContext.defaultParallelism] for the number of partitions `numPartitions`.

Internally, `range` creates a new `Dataset[Long]` with `Range` link:spark-sql-LogicalPlan.adoc[logical plan] and `Encoders.LONG` link:spark-sql-Encoder.adoc[encoder].

=== [[emptyDataFrame]]  Creating Empty DataFrame --  `emptyDataFrame` method

[source, scala]
----
emptyDataFrame: DataFrame
----

`emptyDataFrame` creates an empty `DataFrame` (with no rows and columns).

It calls <<createDataFrame, createDataFrame>> with an empty `RDD[Row]` and an empty schema link:spark-sql-StructType.adoc[StructType(Nil)].

=== [[createDataFrame]] Creating DataFrames from RDDs with Explicit Schema -- `createDataFrame` method

[source, scala]
----
createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame
----

`createDataFrame` creates a `DataFrame` using `RDD[Row]` and the input `schema`. It is assumed that the rows in `rowRDD` all match the `schema`.

=== [[sql]] Executing SQL Queries -- `sql` method

[source, scala]
----
sql(sqlText: String): DataFrame
----

`sql` executes the `sqlText` SQL statement.

```
scala> sql("SHOW TABLES")
res0: org.apache.spark.sql.DataFrame = [tableName: string, isTemporary: boolean]

scala> sql("DROP TABLE IF EXISTS testData")
res1: org.apache.spark.sql.DataFrame = []

// Let's create a table to SHOW it
spark.range(10).write.option("path", "/tmp/test").saveAsTable("testData")

scala> sql("SHOW TABLES").show
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
| testdata|      false|
+---------+-----------+
```

Internally, it creates a link:spark-sql-dataset.adoc[Dataset] using the current `SparkSession` and a link:spark-sql-LogicalPlan.adoc[logical plan]. The plan is created by parsing the input `sqlText` using <<sessionState, sessionState.sqlParser>>.

CAUTION: FIXME See link:spark-sql-sqlcontext.adoc#sql[Executing SQL Queries].

=== [[udf]] Accessing UDF Registration Interface -- `udf` Attribute

[source, scala]
----
udf: UDFRegistration
----

`udf` attribute gives access to `UDFRegistration` that allows registering link:spark-sql-udfs.adoc[user-defined functions] for SQL-based query expressions.

[source, scala]
----
val spark: SparkSession = ...
spark.udf.register("myUpper", (s: String) => s.toUpperCase)

val strs = ('a' to 'c').map(_.toString).toDS
strs.registerTempTable("strs")

scala> sql("SELECT *, myUpper(value) UPPER FROM strs").show
+-----+-----+
|value|UPPER|
+-----+-----+
|    a|    A|
|    b|    B|
|    c|    C|
+-----+-----+
----

Internally, it is an alias for link:spark-sql-sessionstate.adoc#udf[SessionState.udf].

=== [[table]] Creating DataFrames from Tables -- `table` method

[source, scala]
----
table(tableName: String): DataFrame
----

`table` creates a link:spark-sql-dataframe.adoc[DataFrame] from records in the `tableName` table (if exists).

[source, scala]
----
val df = spark.table("mytable")
----

=== [[catalog]] Accessing Metastore -- `catalog` Attribute

[source, scala]
----
catalog: Catalog
----

`catalog` attribute is a (lazy) interface to the current metastore, i.e. link:spark-sql-Catalog.adoc[data catalog] (of relational entities like databases, tables, functions, table columns, and temporary views).

TIP: All methods in `Catalog` return `Datasets`.

[source, scala]
----
scala> spark.catalog.listTables.show
+------------------+--------+-----------+---------+-----------+
|              name|database|description|tableType|isTemporary|
+------------------+--------+-----------+---------+-----------+
|my_permanent_table| default|       null|  MANAGED|      false|
|              strs|    null|       null|TEMPORARY|       true|
+------------------+--------+-----------+---------+-----------+
----

Internally, `catalog` creates a link:spark-sql-Catalog.adoc#CatalogImpl[CatalogImpl] (referencing the current `SparkSession`).

=== [[read]] Accessing DataFrameReader -- `read` method

[source, scala]
----
read: DataFrameReader
----

`read` method returns a link:spark-sql-dataframereader.adoc[DataFrameReader] that is used to read data from external storage systems and load it into a `DataFrame`.

[source, scala]
----
val spark: SparkSession = // create instance
val dfReader: DataFrameReader = spark.read
----

=== [[conf]] Runtime Configuration -- `conf` attribute

[source, scala]
----
conf: RuntimeConfig
----

`conf` returns the current runtime configuration (as `RuntimeConfig`) that wraps link:spark-sql-SQLConf.adoc[SQLConf].

CAUTION: FIXME

=== [[sessionState]] `sessionState` Property

`sessionState` is a transient lazy value that represents the current link:spark-sql-sessionstate.adoc[SessionState].

NOTE: `sessionState` is a `private[sql]` value so you can only access it in a code inside `org.apache.spark.sql` package.

`sessionState` is a lazily-created value based on the internal link:spark-sql-settings.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] setting that can be:

* `org.apache.spark.sql.hive.HiveSessionState` for `hive`
* `org.apache.spark.sql.internal.SessionState` for `in-memory`

=== [[readStream]] `readStream` method

[source, scala]
----
readStream: DataStreamReader
----

`readStream` returns a new link:spark-sql-streaming-DataStreamReader.adoc[DataStreamReader].

=== [[streams]] `streams` Attribute

[source, scala]
----
streams: StreamingQueryManager
----

`streams` attribute gives access to link:spark-sql-streaming-StreamingQueryManager.adoc[StreamingQueryManager] (through link:spark-sql-sessionstate.adoc#streamingQueryManager[SessionState]).

[source, scala]
----
val spark: SparkSession = ...
spark.streams.active.foreach(println)
----

=== [[streamingQueryManager]] `streamingQueryManager` Attribute

`streamingQueryManager` is...

=== [[listenerManager]] `listenerManager` Attribute

`listenerManager` is...

=== [[ExecutionListenerManager]] `ExecutionListenerManager`

`ExecutionListenerManager` is...

=== [[functionRegistry]] `functionRegistry` Attribute

`functionRegistry` is...

=== [[experimentalMethods]] `experimentalMethods` Attribute

[source, scala]
----
experimental: ExperimentalMethods
----

`experimentalMethods` is an extension point with `ExperimentalMethods` that is a per-session collection of extra strategies and ``Rule[LogicalPlan]``s.

NOTE: `experimental` is used in link:spark-sql-SparkPlanner.adoc[SparkPlanner] and link:spark-sql-catalyst-Optimizer.adoc#SparkOptimizer[SparkOptimizer]. Hive and link:spark-sql-structured-streaming.adoc[Structured Streaming] use it for their own extra strategies and optimization rules.

=== [[newSession]] `newSession` method

[source, scala]
----
newSession(): SparkSession
----

`newSession` creates (starts) a new `SparkSession` (with the current link:spark-sparkcontext.adoc[SparkContext] and <<SharedState, SharedState>>).

[source, scala]
----
scala> println(sc.version)
2.0.0-SNAPSHOT

scala> val newSession = spark.newSession
newSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@122f58a
----

=== [[sharedState]] `sharedState` Attribute

`sharedState` is the current <<SharedState, SharedState>>. It is created lazily when first accessed.

=== [[SharedState]] `SharedState`

`SharedState` is an internal class that holds the shared state across active SQL sessions (as <<SparkSession, SparkSession>> instances) by sharing link:spark-sql-CacheManager.adoc[CacheManager], link:spark-webui-SQLListener.adoc[SQLListener], and link:spark-sql-ExternalCatalog.adoc[ExternalCatalog].

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.sql.internal.SharedState` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.internal.SharedState=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

`SharedState` requires a link:spark-sparkcontext.adoc[SparkContext] when created. It also adds `hive-site.xml` to link:spark-sparkcontext.adoc#hadoopConfiguration[Hadoop's `Configuration` in the current SparkContext] if found on CLASSPATH.

NOTE: `hive-site.xml` is an optional Hive configuration file when working with Hive in Spark.

The fully-qualified class name is `org.apache.spark.sql.internal.SharedState`.

`SharedState` is created lazily, i.e. when first accessed after <<creating-instance, `SparkSession` is created>>. It can happen when a <<newSession, new session is created>> or when the shared services are accessed. It is created with a link:spark-sparkcontext.adoc[SparkContext].

When created, `SharedState` sets `hive.metastore.warehouse.dir` to link:spark-sql-settings.adoc#spark_sql_warehouse_dir[spark.sql.warehouse.dir] if `hive.metastore.warehouse.dir` is not set or `spark.sql.warehouse.dir` is set. Otherwise, when `hive.metastore.warehouse.dir` is set and `spark.sql.warehouse.dir` is not, `spark.sql.warehouse.dir` gets set to `hive.metastore.warehouse.dir`. You should see the following INFO message in the logs:

```
INFO spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('[hiveWarehouseDir]').
```

You should see the following INFO message in the logs:

```
INFO SharedState: Warehouse path is '[warehousePath]'.
```

=== [[stop]] Stopping SparkSession -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` stops the `SparkSession`, i.e. link:spark-sparkcontext.adoc#stop[stops the underlying `SparkContext`].

=== [[creating-instance]] Creating `SparkSession` Instance

CAUTION: FIXME

=== [[baseRelationToDataFrame]] `baseRelationToDataFrame` Method

CAUTION: FIXME
