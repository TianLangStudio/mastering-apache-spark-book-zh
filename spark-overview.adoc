== Apache Spark

http://spark.apache.org/[Apache Spark]  是一个优先使用内存的通用开源分布式集群计算框架
可以用它对大量数据做：

* 数据抽取加载（ETL）
* 数据分析（analytics）
* 机器学习 （machine learning）
* 图计算 （graph processing）

支持批处理(batch processing) 和实时处理(streaming processing) + 
支持多种语言调用: Scala, Python, Java, R, and SQL.


.Spark平台
image::diagrams/spark-platform.png[align="center"]

与基于磁盘俩阶段运算的Hadoop MapReduce计算引擎相比，Spark基于内存多阶段运算的数据处理方式通常能提供更高效的性能,如做迭代运算，数据挖掘(read https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html[Spark officially sets a new record in large-scale sorting]).

Spark关注处理速度, 易用性, 可扩展性 及 交互式分析.

Spark也经常被称为 *集群式计算引擎* ， *执行引擎*.

Spark是一个很适合运行 类似 *机器学习算法*，*即席查询* 的复杂多阶段运算任务的分布式计算平台，Spark使用RDDlink:spark-rdd.adoc[弹性分别式数据集 Resilient Distributed Dataset]描述这样的任务 
使用Spark可以简化机器学习，预测性分析类应用的开发。

虽然Spark本身主要由http://scala-lang.org/[Scala]编写, 但是也支持Java, Python, and R 调用

NOTE: 微软的 https://github.com/Microsoft/Mobius[Mobius 项目] 为Spark了提供C#调用支持。
如果典型的MapReduce计算引擎已经不能满足您对数据量和处理时间的要求，你可以考虑试试Spark

* 没有数据类型限制，不依赖特定的数据源.
* 可以处理巨量的数据.

Apache Spark项目包含以下子项目： 

* http://spark.apache.org/sql/[SQL] (使用Datasets)
* http://spark.apache.org/streaming/[streaming]
* http://spark.apache.org/mllib/[machine learning] (pipelines)
* http://spark.apache.org/graphx/[graph] 
他们都构建在Spark Core之上并提供统一的访问方式，你可以把他们当成一个应用

Spark可以运行在你的个人电脑上也可以运行在集群上，可以运行在 Hadoop YARN, Apache Mesos也可以运行在共有云上(Amazon EC2 or IBM Bluemix).

Spark可以访问多种数据源link:spark-data-sources.adoc[data sources].

Apache Spark的 Streaming 和 SQL 编程模型能够帮助开发人员和数据科学家更容易的创建机器学习和图分析应用 

简单的说，所有的Spark应用都是１．先装载数据创建 *RDDs*  2. 然后对ＲＤＤ执行各种link:spark-rdd.adoc[转换操作]变换ＲＤＤ形式　３．最后对ＲＤＤ执行link:spark-rdd.adoc[求值操作]获取运算结果。是不是很简单？！

You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to.

It is Spark's goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine.

NOTE: When you hear "Apache Spark" it can be two things -- the Spark engine aka *Spark Core* or the Apache Spark open source project which is an "umbrella" term for Spark Core and the accompanying Spark Application Frameworks, i.e. link:spark-sql.adoc[Spark SQL], link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX] that sit on top of Spark Core and the main data abstraction in Spark called link:spark-rdd.adoc[RDD - Resilient Distributed Dataset].

=== [[why-spark]] Why Spark

Let's list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand.

==== Easy to Get Started

Spark offers link:spark-shell.adoc[spark-shell] that makes for a very easy head start to writing and running Spark applications on the command line on your laptop.

You could then use link:spark-standalone.adoc[Spark Standalone] built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset.

==== Unified Engine for Diverse Workloads

As said by Matei Zaharia - the author of Apache Spark - in https://youtu.be/49Hr5xZyTEA[Introduction to AmpLab Spark Internals video] (quoting with few changes):

> One of the Spark project goals was to deliver a platform that supports a very wide array of *diverse workflows* - not only MapReduce *batch* jobs (there were available in Hadoop already at that time), but also *iterative computations* like graph algorithms or Machine Learning.
>
> And also different scales of workloads from sub-second interactive jobs to jobs that run for many hours.

Spark combines batch, interactive, and streaming workloads under one rich concise API.

Spark supports *near real-time streaming workloads* via link:spark-streaming/spark-streaming.adoc[Spark Streaming] application framework.

ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads.

Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance.

There is also support for interactive workloads using Spark shell.

You should watch the video https://youtu.be/SxAxAhn-BDU[What is Apache Spark?] by Mike Olson, Chief Strategy Officer and Co-Founder at Cloudera, who provides a very exceptional overview of Apache Spark, its rise in popularity in the open source community, and how Spark is primed to replace MapReduce as the general processing engine in Hadoop.

==== Leverages the Best in distributed batch data processing

When you think about *distributed batch data processing*, link:varia/spark-hadoop.adoc[Hadoop] naturally comes to mind as a viable solution.

Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine.

For many, Spark is Hadoop++, i.e. MapReduce done in a better way.

And it should *not* come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all.

==== RDD - Distributed Parallel Scala Collections

As a Scala developer, you may find Spark's RDD API very similar (if not identical) to http://www.scala-lang.org/docu/files/collections-api/collections.html[Scala's Collections API].

It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense).

So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender.

==== [[rich-standard-library]] Rich Standard Library

Not only can you use `map` and `reduce` (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development.

It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce.

==== Unified development and deployment environment for all

Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or link:spark-shell.adoc[the Spark shell], or the many Spark Application Frameworks leveraging the concept of link:spark-rdd.adoc[RDD], i.e. link:spark-sql.adoc[Spark SQL], link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX], you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (link:spark-mllib/spark-mllib.adoc[Spark MLlib]), a structured data queries (link:spark-sql.adoc[Spark SQL]) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation.

It's also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project.

==== Interactive Exploration / Exploratory Analytics

It is also called _ad hoc queries_.

Using link:spark-shell.adoc[the Spark shell] you can execute computations to process large amount of data (_The Big Data_). It's all interactive and very useful to explore the data before final production release.

Also, using the Spark shell you can access any link:spark-cluster.adoc[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using `--master`) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, link:spark-streaming/spark-streaming.adoc[Spark Streaming], and Spark GraphX.

Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX).

==== Single Environment

Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform.

You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (Datasets), Spark MLlib (ML Pipelines), Spark GraphX (Graphs) or link:spark-streaming/spark-streaming.adoc[Spark Streaming] (DStreams).

Or use them all in a single application.

The single programming model and execution engine for different kinds of workloads simplify development and deployment architectures.

==== Data Integration Toolkit with Rich Set of Supported Data Sources

Spark can read from many types of data sources -- relational, NoSQL, file systems, etc. -- using many types of data formats - Parquet, Avro, CSV, JSON.

Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications.

==== Tools unavailable then, at your fingertips now

As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice).

Spark embraces many concepts in a single unified development and runtime environment.

* Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling `pipe()`).
* DataFrames from R are available in Scala, Java, Python, R APIs.
* Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib.

This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehouses with link:spark-sql-thrift-server.adoc[Thrift JDBC/ODBC Server] in Spark SQL).

Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too.

==== Low-level Optimizations

Apache Spark uses a link:spark-dagscheduler.adoc[directed acyclic graph (DAG) of computation stages] (aka *execution DAG*). It postpones any processing until really required for actions. Spark's *lazy evaluation* gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more).

Mind the proverb https://en.wiktionary.org/wiki/less_is_more[less is more].

==== Excels at low-latency iterative workloads

Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms.

Many Machine Learning algorithms require plenty of iterations before the result models get optimal, like logistic regression. The same applies to graph algorithms to traverse all the nodes and edges when needed. Such computations can increase their performance when the interim partial results are stored in memory or at very fast solid state drives.

Spark can link:spark-rdd-caching.adoc[cache intermediate data in memory for faster model building and training]. Once the data is loaded to memory (as an initial step), reusing it multiple times incurs no performance slowdowns.

Also, graph algorithms can traverse graphs one connection per iteration with the partial result in memory.

Less disk access and network can make a huge difference when you need to process lots of data, esp. when it is a BIG Data.

==== ETL done easier

Spark gives *Extract, Transform and Load (ETL)* a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem.

Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java).

==== [[unified-api]] Unified Concise High-Level API

Spark offers a *unified, concise, high-level APIs* for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API).

Developers no longer have to learn many different processing engines and platforms, and let the time be spent on mastering framework APIs per use case (atop a single computation engine Spark).

==== Different kinds of data processing using unified API

Spark offers three kinds of data processing using *batch*, *interactive*, and *stream processing* with the unified API and data structures.

==== Little to no disk use for better performance

In the no-so-long-ago times, when the most prevalent distributed computing framework was link:varia/spark-hadoop.adoc[Hadoop MapReduce], you could reuse a data between computation (even partial ones!) only after you've written it to an external storage like link:varia/spark-hadoop.adoc[Hadoop Distributed Filesystem (HDFS)]. It can cost you a lot of time to compute even very basic multi-stage computations. It simply suffers from IO (and perhaps network) overhead.

One of the many motivations to build Spark was to have a framework that is good at data reuse.

Spark cuts it out in a way to keep as much data as possible in memory and keep it there until a job is finished. It doesn't matter how many stages belong to a job. What does matter is the available memory and how effective you are in using Spark API (so link:spark-rdd.adoc[no shuffle occur]).

The less network and disk IO, the better performance, and Spark tries hard to find ways to minimize both.

==== Fault Tolerance included

Faults are not considered a special case in Spark, but obvious consequence of being a parallel and distributed system. Spark handles and recovers from faults by default without particularly complex logic to deal with them.

==== Small Codebase Invites Contributors

Spark's design is fairly simple and the code that comes out of it is not huge comparing to the features it offers.

The reasonably small codebase of Spark invites project contributors - programmers who extend the platform and fix bugs in a more steady pace.

=== [[i-want-more]] Further reading or watching

* (video) https://youtu.be/L029ZNBG7bk[Keynote: Spark 2.0 - Matei Zaharia, Apache Spark Creator and CTO of Databricks]
