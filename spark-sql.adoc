== Spark SQL -- 查询大规模结构化数据

*Spark SQL* 主要用于查询结构化和半结构化数据,使用 link:spark-sql-dataset.adoc[Dataset] 操作数据

从Spark2.0最近的变更来看,Sapark SQL变的越来越重要,功能也越来越多 正在成为主要的数据操作方式

[source, scala]
----
// Found at http://stackoverflow.com/a/32514683/1305344
val dataset = Seq(
   "08/11/2015",
   "09/11/2015",
   "09/12/2015").toDF("date_string")

dataset.registerTempTable("dates")

// Inside spark-shell
scala > sql(
  """SELECT date_string,
        from_unixtime(unix_timestamp(date_string,'MM/dd/yyyy'), 'EEEEE') AS dow
      FROM dates""").show
+-----------+--------+
|date_string|     dow|
+-----------+--------+
| 08/11/2015| Tuesday|
| 09/11/2015|  Friday|
| 09/12/2015|Saturday|
+-----------+--------+
----

像SQL和NOSQL数据苦,Spark SQL也提供查询优化,Spark SQL的查询优化是通过Catalyst实现的.优化主要有:
 - link:spark-sql-catalyst-Optimizer.adoc[逻辑语法树优化]
 - link:spark-sql-whole-stage-codegen.adoc[代码生成n] (自动生成的代码通常比你手写的代码更高效)  - link:spark-sql-tungsten.adoc[Tungsten execution engine] 
 - link:spark-sql-InternalRow.adoc[Internal Binary Row Format].

Spark SQL用 link:spark-sql-dataset.adoc[Dataset] (以前叫 link:spark-sql-dataframe.adoc[DataFrame])表示类表类数据. ``Dataset`` 被设计用来在Spark上简单高效的处理类表类数据.

[NOTE]
====
关于 https://drill.apache.org/[Apache Drill] 的表述同样也适用于Spark SQL:

> 一个不需要特别指定原数据的既适用于关系性数据库又适用于NoSQL数据库,既可以查询结构化数据又可以查询非结构数据(JSON,Parquet,HBase)的查询引擎.
====

来一段代码展示 一个简单的ETL流程 从一个JSON文件中读取部分数据另存为CSV文件.

[source, scala]
----
spark.read
  .format("json")
  .load("input-json")
  .select("name", "score")
  .where($"score" > 15)
  .write
  .format("csv")
  .save("output-csv")
----

使用 link:spark-sql-structured-streaming.adoc[结构化数据流] 功能可以将静态的批处理查询转化为动态延续的*实时应用* , 上代码:

[source, scala]
----
import org.apache.spark.sql.types._
val schema = StructType(
  StructField("id", LongType, nullable = false) ::
  StructField("name", StringType, nullable = false) ::
  StructField("score", DoubleType, nullable = false) :: Nil)

spark.readStream
  .format("json")
  .schema(schema)
  .load("input-json")
  .select("name", "score")
  .where('score > 15)
  .writeStream
  .format("console")
  .start

// -------------------------------------------
// Batch: 1
// -------------------------------------------
// +-----+-----+
// | name|score|
// +-----+-----+
// |Jacek| 20.5|
// +-----+-----+
----

As of Spark 2.0, the main data abstraction of Spark SQL is link:spark-sql-dataset.adoc[Dataset]. It represents a *structured data* which are records with a known schema. This structured data representation `Dataset` enables link:spark-sql-tungsten.adoc[compact binary representation] using compressed columnar format that is stored in managed objects outside JVM's heap. It is supposed to speed computations up by reducing memory usage and GCs.

Spark SQL supports link:spark-sql-catalyst-optimizer-PushDownPredicate.adoc[predicate pushdown] to optimize performance of Dataset queries and can also link:spark-sql-catalyst-Optimizer.adoc[generate optimized code at runtime].

Spark SQL comes with the different APIs to work with:

1. link:spark-sql-dataset.adoc[Dataset API] (formerly link:spark-sql-dataframe.adoc[DataFrame API]) with a strongly-typed LINQ-like Query DSL that Scala programmers will likely find very appealing to use.
2. link:spark-sql-structured-streaming.adoc[Structured Streaming API (aka Streaming Datasets)] for continuous incremental execution of structured queries.
3. Non-programmers will likely use SQL as their query language through direct integration with Hive
4. JDBC/ODBC fans can use JDBC interface (through link:spark-sql-thrift-server.adoc[Thrift JDBC/ODBC Server]) and connect their tools to Spark's distributed query engine.

Spark SQL comes with a uniform interface for data access in distributed storage systems like Cassandra or HDFS (Hive, Parquet, JSON) using specialized link:spark-sql-dataframereader.adoc[DataFrameReader] and link:spark-sql-dataframewriter.adoc[DataFrameWriter] objects.

Spark SQL allows you to execute SQL-like queries on large volume of data that can live in Hadoop HDFS or Hadoop-compatible file systems like S3. It can access data from different data sources - files or tables.

Spark SQL defines three types of functions:

* link:spark-sql-functions.adoc[Built-in functions] or link:spark-sql-udfs.adoc[User-Defined Functions (UDFs)] that take values from a single row as input to generate a single return value for every input row.
* link:spark-sql-aggregation.adoc[Aggregate functions] that operate on a group of rows and calculate a single return value per group.
* link:spark-sql-windows.adoc[Windowed Aggregates (Windows)] that operate on a group of rows and calculate a single return value for each row in a group.

There are two supported *catalog* implementations -- `in-memory` (default) and `hive` -- that you can set using link:spark-sql-settings.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] setting.

From user@spark:

> If you already loaded csv data into a dataframe, why not register it as a table, and use Spark SQL
to find max/min or any other aggregates? SELECT MAX(column_name) FROM dftable_name ... seems natural.

> you're more comfortable with SQL, it might worth registering this DataFrame as a table and generating SQL query to it (generate a string with a series of min-max calls)

You can parse data from external data sources and let the _schema inferencer_ to deduct the schema.

[source, scala]
----
// Example 1
val df = Seq(1 -> 2).toDF("i", "j")
val query = df.groupBy('i)
  .agg(max('j).as("aggOrdering"))
  .orderBy(sum('j))
  .as[(Int, Int)]
query.collect contains (1, 2) // true

// Example 2
val df = Seq((1, 1), (-1, 1)).toDF("key", "value")
df.createOrReplaceTempView("src")
scala> sql("SELECT IF(a > 0, a, 0) FROM (SELECT key a FROM src) temp").show
+-------------------+
|(IF((a > 0), a, 0))|
+-------------------+
|                  1|
|                  0|
+-------------------+
----

=== [[i-want-more]] Further reading or watching

1. http://spark.apache.org/sql/[Spark SQL] home page
1. (video) https://youtu.be/e-Ys-2uVxM0?t=6m44s[Spark's Role in the Big Data Ecosystem - Matei Zaharia]
2. https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html[Introducing Apache Spark 2.0]
