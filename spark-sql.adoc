== Spark SQL -- 查询大规模结构化数据

*Spark SQL* 主要用于查询结构化和半结构化数据,使用 link:spark-sql-dataset.adoc[Dataset] 操作数据

从Spark2.0最近的变更来看,Sapark SQL变的越来越重要,功能也越来越多 正在成为主要的数据操作方式

[source, scala]
----
// Found at http://stackoverflow.com/a/32514683/1305344
val dataset = Seq(
   "08/11/2015",
   "09/11/2015",
   "09/12/2015").toDF("date_string")

dataset.registerTempTable("dates")

// Inside spark-shell
scala > sql(
  """SELECT date_string,
        from_unixtime(unix_timestamp(date_string,'MM/dd/yyyy'), 'EEEEE') AS dow
      FROM dates""").show
+-----------+--------+
|date_string|     dow|
+-----------+--------+
| 08/11/2015| Tuesday|
| 09/11/2015|  Friday|
| 09/12/2015|Saturday|
+-----------+--------+
----

像SQL和NOSQL数据苦,Spark SQL也提供查询优化,Spark SQL的查询优化是通过Catalyst实现的.优化主要有:
 - link:spark-sql-catalyst-Optimizer.adoc[逻辑语法树优化]
 - link:spark-sql-whole-stage-codegen.adoc[代码生成n] (自动生成的代码通常比你手写的代码更高效)  - link:spark-sql-tungsten.adoc[Tungsten execution engine] 
 - link:spark-sql-InternalRow.adoc[Internal Binary Row Format].

Spark SQL用 link:spark-sql-dataset.adoc[Dataset] (以前是 link:spark-sql-dataframe.adoc[DataFrame])表示类表类数据. ``Dataset`` 被设计用来在Spark上简单高效的处理类表类数据.

[NOTE]
====
关于 https://drill.apache.org/[Apache Drill] 的表述同样也适用于Spark SQL:

> 一个不需要特别指定原数据的既适用于关系性数据库又适用于NoSQL数据库,既可以查询结构化数据又可以查询非结构数据(JSON,Parquet,HBase)的查询引擎.
====

来一段代码展示 一个简单的ETL流程 从一个JSON文件中读取部分数据另存为CSV文件.

[source, scala]
----
spark.read
  .format("json")
  .load("input-json")
  .select("name", "score")
  .where($"score" > 15)
  .write
  .format("csv")
  .save("output-csv")
----

使用 link:spark-sql-structured-streaming.adoc[结构化数据流] 功能可以将静态的批处理查询转化为动态延续的*实时应用* , 上代码:

[source, scala]
----
import org.apache.spark.sql.types._
val schema = StructType(
  StructField("id", LongType, nullable = false) ::
  StructField("name", StringType, nullable = false) ::
  StructField("score", DoubleType, nullable = false) :: Nil)

spark.readStream
  .format("json")
  .schema(schema)
  .load("input-json")
  .select("name", "score")
  .where('score > 15)
  .writeStream
  .format("console")
  .start

// -------------------------------------------
// Batch: 1
// -------------------------------------------
// +-----+-----+
// | name|score|
// +-----+-----+
// |Jacek| 20.5|
// +-----+-----+
----

在Spark2.0里最主要的数据操作对象是 link:spark-sql-dataset.adoc[Dataset]. DataSet表示已知模式(schema)信息的 *结构化数据* . Dataset表示的结构化数据可以使用 link:spark-sql-tungsten.adoc[compact binary representation] 列压缩格式保存到Java虚拟你的堆外内存中以减少内存占用和垃圾回收时间.

Spark SQL支持 link:spark-sql-catalyst-optimizer-PushDownPredicate.adoc[过滤下推(predicate pushdown)] 和  link:spark-sql-catalyst-Optimizer.adoc[运行时生成优化代码(generate optimized code at runtime)]以提高查询执行效率.

Spark SQL提高的API:

1. link:spark-sql-dataset.adoc[Dataset API] (类似 link:spark-sql-dataframe.adoc[DataFrame API]) 强类型一致的使用方法跟LINQ很像.
2. link:spark-sql-structured-streaming.adoc[Structured Streaming API (Streaming Datasets)] 用于持续增量查询
3. 如果你不想编写代码可以直接使用SQL,Spark SQL良好的集成了Hive
4. 如果你喜欢用JDBC 可以使用 (through link:spark-sql-thrift-server.adoc[Thrift JDBC/ODBC Server]) 以JDBC的方式连接Spark分布式查询引擎.

Spark SQL提供了访问分布式存储上(如 Cassandra,HDFS(Hive,Parquet,JSON))数据的(形式)统一的访问接口 link:spark-sql-dataframereader.adoc[DataFrameReader] 和link:spark-sql-dataframewriter.adoc[DataFrameWriter].


Spark SQL定义了三种类型的函数:

* link:spark-sql-functions.adoc[内建函数] 和  link:spark-sql-udfs.adoc[用户定义函数 (UDFs)]: 以单行数据为参数并为每行数据返回一个处理结果.
* link:spark-sql-aggregation.adoc[聚合函数]: 以一组(包括多行)数据为参数并为每组数据返回一个处理结果
* link:spark-sql-windows.adoc[窗口聚合函数 (Windows)]: 以一组(包含多行)数据为参数并为每组数据的每一行都返回一个处理结果

支持两种元信息(catalog)存储方式 -- 1.`保存在内存种` (默认方式) 2.`使用Hive的元信息` -- 可以通过 link:spark-sql-settings.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] 设置 使用的元信息存储方式.

实例代码:
[source, scala]
----
// Example 1
val df = Seq(1 -> 2).toDF("i", "j")
val query = df.groupBy('i)
  .agg(max('j).as("aggOrdering"))
  .orderBy(sum('j))
  .as[(Int, Int)]
query.collect contains (1, 2) // true

// Example 2
val df = Seq((1, 1), (-1, 1)).toDF("key", "value")
df.createOrReplaceTempView("src")
scala> sql("SELECT IF(a > 0, a, 0) FROM (SELECT key a FROM src) temp").show
+-------------------+
|(IF((a > 0), a, 0))|
+-------------------+
|                  1|
|                  0|
+-------------------+
----

=== [[i-want-more]] 更多信息请查看:

1. http://spark.apache.org/sql/[Spark SQL]主页
1. (video) https://youtu.be/e-Ys-2uVxM0?t=6m44s[Spark在大数据生态中承担的角色 - Matei Zaharia]
2. https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html[Spark2.0介绍]
